# This file is automatically generated by EBNFParser.
from Ruikowa.ObjectRegex.Tokenizer import unique_literal_cache_pool, regex_matcher, char_matcher, str_matcher, Tokenizer
from Ruikowa.ObjectRegex.Node import AstParser, Ref, SeqParser, LiteralValueParser, LiteralNameParser, Undef
namespace = globals()
recur_searcher = set()
token_table = ((unique_literal_cache_pool["space"], regex_matcher('\s+')),
               (unique_literal_cache_pool["number"], regex_matcher('\d+')),
               (unique_literal_cache_pool["symbol"], regex_matcher('[a-zA-Z_]{1}[a-zA-Z_0-9]*')),
               (unique_literal_cache_pool["keyword"], str_matcher(('sub', 'mul', 'div', 'add'))),
               (unique_literal_cache_pool["auto_const"], char_matcher(('-', ')', '('))),
               (unique_literal_cache_pool["auto_const"], str_matcher(('sub', 'mul', 'div', 'add'))),
               (unique_literal_cache_pool["auto_const"], char_matcher((';'))))

class UNameEnum:
# names
    factor = unique_literal_cache_pool['factor']
    mulOrDiv = unique_literal_cache_pool['mulOrDiv']
    keyword = unique_literal_cache_pool['keyword']
    space = unique_literal_cache_pool['space']
    number = unique_literal_cache_pool['number']
    statements = unique_literal_cache_pool['statements']
    auto_const = unique_literal_cache_pool['auto_const']
    arith = unique_literal_cache_pool['arith']
    symbol = unique_literal_cache_pool['symbol']
    atom = unique_literal_cache_pool['atom']
# values
    auto_const_add = unique_literal_cache_pool['add']
    auto_const_div = unique_literal_cache_pool['div']
    auto_const_mul = unique_literal_cache_pool['mul']
    keyword_add = unique_literal_cache_pool['add']
    auto_const_sub = unique_literal_cache_pool['sub']
    keyword_sub = unique_literal_cache_pool['sub']
    keyword_div = unique_literal_cache_pool['div']
    keyword_mul = unique_literal_cache_pool['mul']
        
cast_map = {
    'add': UNameEnum.keyword,
    'mul': UNameEnum.keyword,
    'sub': UNameEnum.keyword,
    'div': UNameEnum.keyword
}
        
token_func = lambda _: Tokenizer.from_raw_strings(_, token_table, ({"space"}, {}), cast_map=cast_map)

space = LiteralNameParser('space')
number = LiteralNameParser('number')
symbol = LiteralNameParser('symbol')
keyword = LiteralNameParser('keyword')
atom = AstParser([Ref('symbol')],
                 [Ref('number')],
                 ['(', Ref('arith'), ')'],
                 name="atom",
                 to_ignore=({}, {}))
factor = AstParser([SeqParser(['-'], at_least=0,at_most=1), Ref('atom')],
                   name="factor",
                   to_ignore=({}, {}))
mulOrDiv = AstParser([Ref('factor'), SeqParser([SeqParser(['div'], ['mul'], at_least=1,at_most=1), Ref('factor')], at_least=0,at_most=Undef)],
                     name="mulOrDiv",
                     to_ignore=({}, {}))
arith = AstParser([Ref('mulOrDiv'), SeqParser([SeqParser(['add'], ['sub'], at_least=1,at_most=1), Ref('mulOrDiv')], at_least=0,at_most=Undef)],
                  name="arith",
                  to_ignore=({}, {}))
statements = AstParser([Ref('arith'), SeqParser([';', Ref('arith')], at_least=0,at_most=Undef)],
                       name="statements",
                       to_ignore=({}, {}))
statements.compile(namespace, recur_searcher)